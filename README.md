# Hands-on-transformer

This repository contains step-by-step implementations of the core components of the Transformer architecture in PyTorch.  
It is designed as a learning project to better understand how Transformers work under the hood.

## Files
- `01_self_attention.ipynb` – Implementation of Scaled Dot-Product Self-Attention  
- `02_multi_head_attention.ipynb` – Multi-Head Attention mechanism  
- `03_encoder_layer.ipynb` – Encoder layer with attention and feed-forward network  
- `04_decoder_layer.ipynb` – Decoder layer with self-attention, cross-attention, and feed-forward network

<img width="400" height="800" alt="image" src="Images/transformer.png" />

## Requirements
Reference Environment:
- Python 3.10.18
- PyTorch 2.8.0
- Jupyter Notebook
- ...

## Quick Start
Clone the repository and run the notebooks step by step:

```bash
cd hands-on-transformer
jupyter notebook
```
