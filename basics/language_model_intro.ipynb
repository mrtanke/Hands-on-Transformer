{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9800af8-fb2d-4e54-b2f8-40ecd7afc906",
   "metadata": {},
   "source": [
    "# Three classical Language model catagories\n",
    "\n",
    "### 1️⃣ Autoregressive LM => Continuation (free generation)\n",
    "No intense connection between input and output.\n",
    "### 2️⃣ Autoencoding LM => Understanding (deep analysis)\n",
    "### 3️⃣ Seq2Seq LM => Transformation (X to Y)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Naming conventions\n",
    "### 1.1 Autoregressive LM\n",
    "- \"regression\" comes from statistics:\n",
    "    - using exsting data to predict the next point.\n",
    "- In here, it means predicting the next word using the previous words.\n",
    "### 1.2 Autoencoding LM\n",
    "- \"Autoencoding\" comes from Autoencoder:\n",
    "    - input -> compression -> reconstruction\n",
    "- In here, it means recover the [mask] locations in the sentence.\n",
    "### 1.3 Seq2Seq\n",
    "- input a sequence, output another sequence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf06686-c956-4a80-9e95-666d0a2946b6",
   "metadata": {},
   "source": [
    "## 2. Comparison of Three Language Modeling Paradigms\n",
    "(Autoregressive, Autoencoding, Seq2Seq)\n",
    "\n",
    "### 2.1 Autoregressive LM\n",
    "- **Core idea**: Predict the next token from left to right.\n",
    "- **Formula**:\n",
    "  $$P(x) = ∏_{t=1}^n P(x_t | x_{<t})$$\n",
    "- **Training**:\n",
    "  - Input: \"I like\"\n",
    "  - Output: predict \"playing\"\n",
    "- **Architecture**:\n",
    "  - Decoder-only Transformer\n",
    "- **Strengths**:\n",
    "  - Strong generative ability (dialogue, continuating writing, code)\n",
    "      - *Essentially a continuation*\n",
    "- **Weaknesses**:\n",
    "  - Only unidirectional context, weaker at deep understanding\n",
    "- **Representative models**:\n",
    "  - GPT-1, GPT-2, GPT-3, GPT-4, LLaMA\n",
    "\n",
    "### 2.2 Autoencoding LM\n",
    "- **Core idea**: Mask some tokens, let the model recover them.\n",
    "- **Formula**:\n",
    "  $$L_{MLM} = - ∑_{i ∈ M} log P(x_i | x_{\\setminus M})$$\n",
    "  $$(M = masked\\ positions)$$\n",
    "- **Training**:\n",
    "  - Input: \"I like [MASK] football\"\n",
    "  - Output: predict \"playing\"\n",
    "- **Architecture**:\n",
    "  - Encoder-only Transformer (bidirectional)\n",
    "- **Strengths**:\n",
    "  - Excellent for semantic understanding, classification, QA (extract from original text)\n",
    "- **Weaknesses**:\n",
    "  - Not suited for free-form text generation\n",
    "- **Representative models**:\n",
    "  - BERT, RoBERTa, ELECTRA\n",
    "\n",
    "### 2.3 Seq2Seq LM\n",
    "- **Core idea**: Map an input sequence to an output sequence.\n",
    "- **Formula**:\n",
    "  $$P(y|x) = ∏_{t=1}^m P(y_t | y_{<t}, x)$$\n",
    "- **Training**:\n",
    "  - Input: \"Translate English to German: I like football\"\n",
    "  - Output: \"Ich mag Fußball\"\n",
    "- **Architecture**:\n",
    "  - Encoder-Decoder Transformer\n",
    "- **Strengths**:\n",
    "  - Flexible for tasks like translation, summarization, paraphrasing\n",
    "- **Weaknesses**:\n",
    "  - Training/inference more complex and slower\n",
    "- **Representative models**:\n",
    "  - T5, BART, mBART\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22873b6-6851-4d53-a677-8c90d449e9b5",
   "metadata": {},
   "source": [
    "## 3. Summary Table\n",
    "\n",
    "| Model type       | Core Objective / Train       | Formula                                  | Architecture     | Strengths                 | Weaknesses        | Representative |\n",
    "|------------------|----------------------|------------------------------------------|------------------|---------------------------|-------------------|----------------|\n",
    "| Autoregressive   | Next token prediction| $$P(x)=∏ P(x_t | x_{<t})$$                  | Decoder-only     | Strong text generation    | Weak at deep understanding | GPT family |\n",
    "| Autoencoding     | Masked token recovery| $$L=-∑ log P(x_i | x_{\\setminus M})$$        | Encoder-only     | Strong understanding      | Weak generation   | BERT family |\n",
    "| Seq2Seq          | Input→Output mapping | $$P(y|x)=∏ P(y_t|y_{<t},x)$$                 | Encoder-Decoder  | Translation, summarization| Slower, complex   | T5, BART |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad70f9-47c8-4269-96b5-7c553e3d0609",
   "metadata": {},
   "source": [
    "## 4. In HuggingFace \n",
    "- Specific training objective implementation for Hugging Face.\n",
    "    - **CausalLM** -> AutoregressiveLM\n",
    "    - **MaskedLM** -> AutoencodingLM\n",
    "    - **Encoder-DecoderLM** -> seq2seqLM\n",
    "- These names are specific subclasses of AutoencodingLM / AutoregressiveLM / seq2seqLM in the Hugging Face framework\n",
    "    - Relationship: MaskedLM ≈ an implementation of AutoencodingLM; CausalLM ≈ an implementation of AutoregressiveLM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (bert)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
