{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "622ecefd-0647-454e-afee-36092a23413c",
   "metadata": {},
   "source": [
    "# T5 model\n",
    "Text-to-Text Transfer Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5286dfeb-2fe2-4fe9-845c-9f110e2d6292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e887bf89-43ac-4fe3-8e1d-33746262c67c",
   "metadata": {},
   "source": [
    "## 1. Summary\n",
    "- Overview\n",
    "  - Introduced by Google (2020): \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"\n",
    "  - Unified framework: every NLP task (translation, summarization, QA, classification) is cast as \"text-to-text\"\n",
    "- Architecture\n",
    "  - Encoder–Decoder Transformer (like seq2seq)\n",
    "      - Encoder: processes input text into contextual representations\n",
    "      - Decoder: autoregressively generates output tokens\n",
    "- Usage\n",
    "  - Convert various NLP tasks (translation, classification, summarization, fill-in-the-blank, etc.) into a \"text-to-text\" format.\n",
    "      - For example, a classification task: \"sst2 sentence: this movie is great\" → \"positive\"\n",
    "      - For example, a summarization task: \"summarize: ...\" → \"short summary\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441755c-0297-4b08-bebc-01c4f0069621",
   "metadata": {},
   "source": [
    "## 2. T5 model\n",
    "- t5-small\n",
    "- t5-base\n",
    "- t5-large\n",
    "- t5-3b\n",
    "- t5-11b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b2e36b-5f1f-40f2-b080-8d527eee2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 't5-small'\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e8a94-37de-476a-8664-60d6e8b7e3fa",
   "metadata": {},
   "source": [
    "## 3. Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea68993-fa6c-4210-8768-e31935bb4d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"T5, BERT, GPT2 are all based on Transformer architecture\", return_tensors=\"pt\").input_ids\n",
    "decoder_input_ids = tokenizer(\"T5, BERT, GPT2\", return_tensors=\"pt\").input_ids  \n",
    "\n",
    "# AutoModel -> preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.\n",
    "# T5ForConditionalGeneration -> no preprocess need, as it does this internally using labels arg.\n",
    "decoder_input_ids = model._shift_right(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046e634-d91b-45b3-a03b-cf884f981ffa",
   "metadata": {},
   "source": [
    "### Model parameters\n",
    "- input_ids -> input of encoder\n",
    "- decoder_input_ids -> input of decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c57f6dcc-26b1-49e7-9628-50475d48e0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8644e-02,  1.9237e-01, -1.0070e-02,  ...,  9.7096e-02,\n",
       "           4.8512e-04, -1.3691e-01],\n",
       "         [ 1.1108e-01,  2.3853e-01, -1.7094e-01,  ...,  8.5612e-02,\n",
       "           6.8864e-04, -1.8778e-01],\n",
       "         [-2.4158e-02,  9.6821e-04,  7.7325e-02,  ...,  1.2552e-01,\n",
       "           1.1121e-03, -5.3361e-02],\n",
       "         ...,\n",
       "         [-1.8146e-02,  1.1181e-01, -6.2067e-02,  ...,  9.9619e-02,\n",
       "           6.4774e-04, -5.3506e-01],\n",
       "         [ 4.9392e-02,  2.4820e-01, -5.2307e-02,  ...,  1.8961e-01,\n",
       "           2.2361e-04, -2.3843e-01],\n",
       "         [ 2.9291e-02,  1.8362e-01,  3.1791e-02,  ...,  1.4849e-01,\n",
       "           8.6182e-05, -3.9248e-02]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048029de-3b42-4e8d-8de9-8790a5801507",
   "metadata": {},
   "source": [
    "T5 source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06004da9-2bbb-49c7-b493-0bc745192c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8644e-02,  1.9237e-01, -1.0070e-02,  ...,  9.7096e-02,\n",
       "           4.8512e-04, -1.3691e-01],\n",
       "         [ 1.1108e-01,  2.3853e-01, -1.7094e-01,  ...,  8.5612e-02,\n",
       "           6.8864e-04, -1.8778e-01],\n",
       "         [-2.4158e-02,  9.6821e-04,  7.7325e-02,  ...,  1.2552e-01,\n",
       "           1.1121e-03, -5.3361e-02],\n",
       "         ...,\n",
       "         [-1.8146e-02,  1.1181e-01, -6.2067e-02,  ...,  9.9619e-02,\n",
       "           6.4774e-04, -5.3506e-01],\n",
       "         [ 4.9392e-02,  2.4820e-01, -5.2307e-02,  ...,  1.8961e-01,\n",
       "           2.2361e-04, -2.3843e-01],\n",
       "         [ 2.9291e-02,  1.8362e-01,  3.1791e-02,  ...,  1.4849e-01,\n",
       "           8.6182e-05, -3.9248e-02]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def t5_forward(model, input_ids, decoder_input_ids):\n",
    "    encoder_outputs = model.encoder(input_ids=input_ids)\n",
    "    hidden_states = encoder_outputs[0]\n",
    "    decoder_outputs = model.decoder(input_ids=decoder_input_ids, encoder_hidden_states=hidden_states)\n",
    "\n",
    "    return decoder_outputs.last_hidden_state\n",
    "\n",
    "t5_forward(model, input_ids, decoder_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d78fefe-29b4-40a4-8cd2-68ee26dee151",
   "metadata": {},
   "source": [
    "## 4. Pretrain tasks\n",
    "- Unsupervised denoising training\n",
    "  - MLM (Masked Language Model) -> Add noise to the original sentence, then let the model restore the original sentence\n",
    "      - span mask\n",
    "- Supervised training -> manual labeling mask\n",
    "    - seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a755ff-8f7a-4529-ad42-2cc8410b06d7",
   "metadata": {},
   "source": [
    "The difference between T5ForConditionalGeneration to T5Model:\n",
    "- Add a lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0c7d389-cfab-4c0e-bf3f-803110feccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26b1588d-15a2-4a42-aa6e-dea96d6752db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7837252616882324"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unsupervised denoising training\n",
    "# mlm\n",
    "\n",
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# the forward function automatically creates the correct decoder_input_ids\n",
    "loss = model(input_ids=input_ids, labels=labels).loss\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1a00ac6-4687-4db6-910b-3c4c5e436e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.684732437133789"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Supervised training\n",
    "# seq2seq\n",
    "\n",
    "input_ids = tokenizer(\"translate English to German: What is your name?\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"wie heißt du?\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# the forward function automatically creates the correct decoder_input_ids\n",
    "loss = model(input_ids=input_ids, labels=labels).loss\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb648e2-18fd-4fd7-aeed-839e42cf2b5c",
   "metadata": {},
   "source": [
    "### 4.1 multi sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d6e0bd5-14e5-42d3-b80b-bfff58ea21c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.188005730509758"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_source_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "# Example pairs (English → French)\n",
    "inputs = [\"Welcome to NYC\", \"HuggingFace is a company\"]\n",
    "targets = [\"Bienvenue à NYC\", \"HuggingFace est une entreprise\"]\n",
    "\n",
    "# Add task prefix to inputs (T5 uses text-to-text format)\n",
    "task_prefix = \"translate English to French: \"\n",
    "inputs = [task_prefix + s for s in inputs]\n",
    "\n",
    "# Encode source sentences\n",
    "encoding = tokenizer(\n",
    "    inputs,\n",
    "    padding=\"longest\",              # pad to longest input\n",
    "    max_length=max_source_length,   # truncate if too long\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"             # return PyTorch tensors\n",
    ")\n",
    "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "\n",
    "# Encode target sentences\n",
    "target_encoding = tokenizer(\n",
    "    targets,\n",
    "    padding=\"longest\",\n",
    "    max_length=max_target_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "labels = target_encoding.input_ids\n",
    "\n",
    "# Ignore padding tokens in loss calculation\n",
    "labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "# Forward pass (compute loss)\n",
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "loss = outputs.loss\n",
    "\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6171b667-4ddf-478b-9dd8-d2c6320ddb88",
   "metadata": {},
   "source": [
    "## 5. Specific tasks\n",
    "- model output -> [batch_size, seq_len, vocab_size]\n",
    "- model.generate output -> [batch, seq_len] ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3b87463-d398-4910-8878-d7541429dc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hallo, mein Name ist reven'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\"translate English to German: Hello, my name is reven\", return_tensors='pt').input_ids\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d18b5deb-4746-4e86-a630-5a0f8cd1def9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transfer learning is a powerful technique in natural language processing. the effectiveness of transfer learning has given rise to a diversity of approaches, methodologies, and practice.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inference\n",
    "input_ids = tokenizer(\n",
    "    \"summarize: Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.\", \n",
    "    return_tensors=\"pt\"\n",
    ").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=128)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (bert)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
